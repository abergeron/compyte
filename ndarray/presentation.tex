\documentclass[utf8x,xcolor=pdftex,dvipsnames,table]{beamer}
\usetheme{Malmoe}  % Now it's a beamer presentation with the lisa theme!
\setbeamertemplate{footline}[page number]
\usecolortheme{beaver}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage[utf8x]{inputenc}
%\logo{\includegraphics[width=.8in]{UdeM_NoirBleu_logo_Marie_crop}}

\newcommand{\superscript}[1]{\ensuremath{^{\textrm{#1}}}}

\mode<presentation>

\title{A Common GPU n-Dimensional Array for Python and C}

\author{
\footnotesize
Frédéric Bastien\superscript{a}, Arnaud Bergeron\superscript{a}, Andreas Klöckner\superscript{b}, \newline
Pascal Vincent\superscript{a} and Yoshua Bengio\superscript{a} \newline \newline
\superscript{a}Département d'Informatique et de Recherche Opérationnelle \newline
Université de Montréal \newline
Montréal, Canada \newline
\texttt{\{bastienf, bergearn, vincentp, bengioy\}@iro.umontreal.ca} \newline \newline
\superscript{b}Courant Institute of Mathematical Sciences \newline
New York University\newline
New York, NY 10012, USA\newline
\texttt{kloeckner@cims.nyu.edu}
}

\date{NIPS 2011 Big Learning Workshop}

\setbeamertemplate{navigation symbols}{}

\begin{document}

\begin{frame}[plain]
 \titlepage
 \vspace{-5em}
 \includegraphics[width=1in]{lisabook_logo_text_3.png}
 \hfill
 \includegraphics[width=.8in]{UdeM_NoirBleu_logo_Marie_crop}
\end{frame}

\setcounter{page}{1}

\section{Motivation}

\begin{frame}{Why do we need this?}
\begin{itemize}
\item Efficient linear algebra is a the core of many scientific applications
\item On the CPU, numpy ndarray provides a standard object (for python at least)
\end{itemize}
\end{frame}

\begin{frame}{Why a new implementation?}
\begin{block}{There are already a number of existing GPU computing codebases:}
Theano, PyCUDA/PyOpenCL, CUDAmat, Gnumpy, Thrust, ...
\end {block}
\begin{enumerate}
\item<2-> All are incompatible, which hinders code sharing.
\item<3-> They do not support the full range of numpy ndarray features
\item<4-> None support both CUDA and OpenCL
\end{enumerate}
\end{frame}

\section{Features}
\begin{frame}{Features desired}
\begin{itemize}
\item Support for varying datatypes
\item Support for an arbitrary number of dimensions
\item Support for strides
\item \color{gray!80} Support for broadcasting
\item Compatibility with CUDA and OpenCL
\end{itemize}
\end{frame}

\begin{frame}{Strides}
\only<1>{Strides is a way to specify how much memory to skip between each element of a dimension.}
\only<2>{We can use strides to take submatrix ${\color{cyan!50}B}$ without copying any memory.}
\begin{center}
\onslide<1->{Matrix ${\color{red!50}A}$}\hspace{5em}\onslide<2->{Matrix ${\color{cyan!50}B}$}
\end{center}
\begin{center}
\only<1>{\includegraphics{strides-1}}
\only<2>{\includegraphics{strides-2}}
\end{center}
\end{frame}

\begin{frame}{Features desired}
\begin{itemize}
\item {\color{gray!80} Support for varying datatypes}
\item {\color{gray!80} Support for an arbitrary number of dimensions}
\item {\color{gray!80} Support for strides}
\item Support for broadcasting
\item {\color{gray!80} Compatibility with CUDA and OpenCL}
\end{itemize}
\end{frame}

\begin{frame}{Broadcasting}
\only<1>{We have matrix ${\color{red!50}A}$ (size [8,8]) and we want to add a bias vector ${\color{cyan!50}b}$ (size [8,1]) to it.}
\only<2>{So we make virtual copies of ${\color{cyan!50}b}$ along the last dimension until it has the same size as ${\color{red!50}A}$.}
\begin{center}
\only<1>{\includegraphics[width=0.9\textwidth]{bcast-1}}
\only<2>{\includegraphics[width=0.9\textwidth]{bcast-2}}
\end{center}
\only<1>{This doesn't fit the rules for elementwise operations since both objects do not have the same number of elements.}
\only<2>{Then we can proceed as usual for elementwise. \\ \ }
\end{frame}

\begin{frame}{Features desired}
\begin{itemize}
\item {\color{gray!80} Support for varying datatypes}
\item {\color{gray!80} Support for an arbitrary number of dimensions}
\item {\color{gray!80} Support for strides}
\item {\color{gray!80} Support for broadcasting}
\item Compatibility with CUDA and OpenCL
\end{itemize}
\end{frame}

\begin{frame}
\begin{block}{Why has this not been done before?}
\begin{itemize}
\item Hard and time consuming to get right and efficient
\begin{itemize}
\item \normalsize Indexing computations take up a significant portion of time on the GPU
\end{itemize}
\end{itemize}
\end{block}
\begin{block}{Easy to develop}
\begin{itemize}
\item Not always a good idea to make a gpu code work for all memory layout.
  \begin{itemize}
  \item Harder to code
  \item Harder to get efficient
  \end{itemize}
\item Just call \begin{bf}as\_\{contiguous,fortran\}\_memory()\end{bf}
 on inputs!
\end{itemize}
\end{block}
\end{frame}

\section{Existing Implementations}

\begin{frame}{Comparison of existing implementations}
\begin{table}
\rowcolors{2}{RoyalBlue!5}{RoyalBlue!23}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Package & strides & bcast & dims & types & backends \\
\hline
\hline
Theano & yes\footnote{as number of elements} & yes & any & float32 & CUDA \\
PyCUDA& no & no & any & all & CUDA \\
PyOpenCL & no & no & any & all & OpenCL \\
CUDAMat & no & yes\footnote{via a function} & 2 & float32 & CUDA \\
Gnumpy & no & yes & any & float32\footnote{and a hackish form of boolean} & CUDA \\
Thrust & no & no & 1 & all & CUDA \\
\hline
\hiderowcolors
Desired & yes & yes & any & all & both \\
\hline
\end{tabular}
\end{table}
\end{frame}

\section{Current State}

\begin{frame}{Functionality}
\begin{block}{What we have}
\begin{itemize}
\item data types
\item dimensions
\item strides, views
\item broadcasting
\item elementwise kernels
\item partial reductions
\item support for CUDA and OpenCL
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Functionality (cont.)}
\begin{block}{Interfaces}
\begin{itemize}
\item Python
\item C++ interface similar to Numpy C-API (depends on python)
\end{itemize}
\end{block}
\begin{block}{Missing}
\begin{itemize}
\item assignation
\item reshaping
\item a clean C interface
\end{itemize}
\end{block}
\end{frame}

\subsection{Element-wise dimension collapsing}

\begin{frame}
\begin{itemize}
\item Indexing computations are expensive
\item The cost is paid per dimension (irrespective of their size)
\item We have an optimization that can help alleviate the costs
\end{itemize}
\end{frame}

\begin{frame}
\only<1>{Suppose we have some elementwise work to do on a 3d tensor ${\color{cyan!50}B}$ that is a view of ${\color{red!50}A}$, but strided in the innnermost dimension.}
\only<2>{We can merge the two outer dimensions to obtain an equivalent array that accesses the same memory but with easier indexing.}
\begin{center}
\only<1>{\includegraphics[height=0.8\textheight]{dimcoll-1}}
\only<2>{\includegraphics[width=0.9\textwidth]{dimcoll-2}}
\end{center}
\end{frame}

\section{Benchmarks}
\begin{frame}{Benchmarks}
\vspace{-1em}
\begin{center}
\includegraphics[width=0.85\textwidth]{ap1_no_alloc}
\end{center}
\end{frame}

%\begin{frame}{Benchmarks (cont.)}
%\vspace{-1em}
%\begin{center}
%\includegraphics[width=0.85\textwidth]{a2pb2p2ab_no_alloc}
%\end{center}
%\end{frame}

\section{Conclusion}
\begin{frame}{Future plans}
\begin{itemize}
\item<1-> Use in Theano/PyOpenCL/PyCUDA
\item<2-> Design and implement a good C/C++ interface
\item<3-> Find ways to lower the overhead
\item<4-> Use the implicit looping provided by CUDA and OpenCL
\item<5-> World domination!
\begin{itemize}
\item<6-> Library authors $\to$ come see us
\item<6-> Supervisers $\to$ talk to your students about this project
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Acknowledgment}
\begin{itemize}
\item James Bergstra
\item Compute Canada, RQCHP, NSERC, and Canada Research Chairs for providing funds or access to compute resources.
\end{itemize}
\end{frame}

\begin{frame}
\begin{center}
\Huge
Questions?
\end{center}
\end{frame}


\end{document}
